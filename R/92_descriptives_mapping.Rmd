---
title: "Luxembourg Research Evaluation 2022: Field Mapping of Knowledge Structure"
author: "Daniel S. Hain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: flatly
    code_folding: hide
    df_print: paged
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
params:
    institute: 
       value: null
    department:
       value: null
---

<!---
# Add to YAML when reviewing
  html_notebook:
    theme: flatly
    code_folding: hide
    df_print: paged
    toc: false
    toc_depth: 2
    toc_float:
      collapsed: false
--->


```{=html}
<style type="text/css">
.main-container {
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE}
### Generic preamble
#rm(list=ls())
Sys.setenv(LANG = "en")
options(scipen = 5)
set.seed(1337)

### Load packages  
# general
library(tidyverse)
library(magrittr)

# Kiblio & NW
library(bibliometrix)
library(tidygraph)
library(ggraph)

# NLP
library(tidytext)

# Dataviz
library(plotly)
library(ggforce)
library(ggrepel)
library(patchwork)

# Knit
library(knitr) # For display of the markdown
library(kableExtra) # For table styling

# own functions
source("../functions/functions_basic.R")
source("../functions/functions_summary.R")
source("../functions/00_parameters.R")

# Knitr options
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

```{r, include=FALSE}
#var_inst <- 'LISER'
#var_dept <- 'UD'
```

```{r, include=FALSE}
var_inst <- params$institute
var_dept <- params$department
```

# Introduction: `r var_inst` Department `r var_dept`

Here are preliminary results of the bibliometric mapping of the 2022 Luxembourg research evaluation. Its purpose is:

* To map the broader research community and distinct research field the department contributes to.
* Identify core knowledge bases, research areas gtrends and topics.
* Highlight the positioning of the department within this dynamics.

The method for the research-field-mapping can be reviewed here:

[Rakas, M., & Hain, D. S. (2019). The state of innovation system research: What happens beneath the surface?. Research Policy, 48(9), 103787.](https://doi.org/10.1016/j.respol.2019.04.011)


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

```{r, include=FALSE}
# Load data
M <- readRDS(paste0('../../temp/M_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>% as_tibble() %>% 
  distinct(UT, .keep_all = TRUE) %>% 
  filter(PY >= PY_min, PY <= PY_max) 
```

# Seed Articles

```{r, include=FALSE}
seed <-convert2df(file = paste0('../../data/seeds/scopus_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '_seed_select.csv'), dbsource = "scopus", format = "csv") %>%
  as_tibble() %>%
  mutate(seed = TRUE) 
```

The seed articles deemed representative for the active areas of research in the institution, and include authors affiliated with the institution. They can be selected in three ways:

1. Via bibliographic clustering of the institutions publications and selection of most central articles per cluster (only clsuters where n >= 0.05N). Selection can be found at: `r paste0('https://github.com/daniel-hain/biblio_lux_2022/blob/master/output/seed/scopus_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '_seed.csv')`
2. Manual selection of relevant publications.
3. A combination of 1. and 2.

The present analysis is based on the following seed articles:

```{r}
seed %>%
  select(AU, PY, TI, JI) %>%
  mutate(AU = AU %>% str_trunc(30),
         TI = TI %>% str_trunc(100),
         JI = JI %>% str_trunc(30)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 10)
```

```{r}
com_labels <- read_csv2('../../data/community_labeling.csv')  %>% filter(institute == var_inst, department == var_dept) %>% arrange(institute, department, type, com) %>% mutate(label = ifelse(is.na(label), paste0(type, ' ', com, ': unlabeled'), paste0(type, ' ', com, ': ', label)))
```

# Topic modelling {.tabset}

Here, we report the results of a LDA topic-modelling (basically, clustering on words) on all title+abstract texts.

```{r, include=FALSE}
text_tidy <- readRDS(paste0('../../temp/text_tidy_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))
text_lda <- readRDS(paste0('../../temp/text_LDA_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) 

text_lda_beta <- text_lda %>% tidy(matrix = "beta") 
text_lda_gamma <- text_lda %>% tidy(matrix = "gamma")
```

```{r, include=FALSE}
com_names_top <- tibble( 
  com = 1:(text_lda_gamma %>% pull(topic) %>% n_distinct()),
  type = 'TP',
  col = com %>% gg_color_select(pal = pal_tp)) %>%
  left_join(com_labels %>% filter(type == 'TP') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('TP ', com, ': unlabeled'), label))
            
# # 1st alternative: Number them 1-n
# paste(type, 1:(text_lda_gamma %>% pull(topic) %>% n_distinct()))           
```

```{r, include=FALSE}
text_lda_beta %<>%  inner_join(com_names_top %>% select(com, label, col), by = c('topic' = 'com'))
text_lda_gamma %<>% inner_join(com_names_top %>% select(com, label, col), by = c('topic' = 'com'))
```


## Topics by topwords

```{r, fig.width=15, fig.height=15} 
text_lda_beta %>%
  group_by(label) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, label)) %>%
  ggplot(aes(term, beta, fill = factor(label))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ label, scales = "free", ncol = 3) +
  coord_flip() +
  scale_x_reordered() +
  labs(x = "Intra-topic distribution of word",
       y = "Words in topic") + 
  scale_fill_manual(name = "Legend", values = com_names_top %>% pull(col)) +
  theme(legend.position = 'bottom')

#plot_ly <- plot %>% plotly::ggplotly()
#htmlwidgets::saveWidget(plotly::as_widget(plot_ly), '../output\vis_plotly_topic_terms.html', selfcontained = TRUE)
```




**Note:** While this static vies is helpful, I recommend using the interactive LDAVis version to be found under `r paste0('https://daniel-hain.github.io/biblio_lux_2022/output/topic_modelling/LDAviz_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds/index.html#topic=1&lambda=0.60&term=')`. For functionality and usage, see technical description in the next tab.

## Topics over time

```{r, fig.width = 15, fig.height=7.5}
text_lda_gamma %>%
  rename(weight = gamma) %>%
  left_join(M %>% select(XX, PY), by = c('document' = 'XX')) %>%
  mutate(PY = as.numeric(PY)) %>%
  group_by(PY, label) %>% summarise(weight = sum(weight)) %>% ungroup() %>%
  group_by(PY) %>% mutate(weight_PY = sum(weight)) %>% ungroup() %>%
  mutate(weight_rel = weight / weight_PY) %>%
  select(PY, label, weight, weight_rel) %>%
  filter(PY >= PY_min & PY <= PY_max) %>%
  arrange(PY, label) %>%
  plot_summary_timeline(y1 = weight, y2 = weight_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label,  label = TRUE, pal = pal_tp, 
                        y1_text = "Topic popularity annualy", y2_text = "Share of topic annually") +
  plot_annotation(title = paste('Topic Modelling:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute topic appearance (left), Relative topic appearance (right)')
```


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

```{r, include=FALSE}
rm(text_tidy, text_lda)
```


## Technical Description

### LDA Topic Modelling

Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA, [Blei et al., 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com)) is an example of topic model and is used to classify text in a document to a particular topic. 

LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.

### LDAVis

[LDAvis](https://github.com/cpsievert/LDAvis) is a web-based interactive visualisation of topics estimated using LDA ([Sievert & Shirley, 2014](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)). It provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization. The visualisation has two basic pieces.

The **left panel** visualise the topics as circles in the two-dimensional plane whose centres are determined by computing the Jensen–Shannon divergence between topics, and then by using multidimensional scaling to project the inter-topic distances onto two dimensions. Each topic’s overall prevalence is encoded using the areas of the circles.

The **right panel** depicts a horizontal bar chart whose bars represent the individual terms that are the most useful for interpreting the currently selected topic on the left. A pair of overlaid bars represent both the corpus-wide frequency of a given term as well as the topic-specific frequency of the term.

The $\lambda$ slider allows to rank the terms according to term relevance. By default, the terms of a topic are ranked in decreasing order according their topic-specific probability ( $\lambda$ = 1 ). Moving the slider allows to adjust the rank of terms based on much discriminatory (or "relevant") are for the specific topic. The suggested optimal value of $\lambda$ is 0.6.


# Knowledge Bases: Co-Citation network analysis {.tabset}

```{r, include=FALSE}
C_nw <- readRDS(paste0('../../temp/C_nw_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))  %>%
  drop_na(com)
```

```{r, include=FALSE}
com_names_cit <- tibble( 
  com = 1:(C_nw %>% pull(com) %>% n_distinct()),
  type = 'KB',
  col = com %>% gg_color_select(pal = pal_kb)) %>%
  left_join(com_labels %>% filter(type == 'KB') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('KB ', com, ': unlabeled'), label))

# # 1st alternative: Number them 1-n
# paste(type, 1:(C_nw %>% pull(com) %>% n_distinct()))
```

```{r, include=FALSE}
C_nw %<>% left_join(com_names_cit %>% select(com, label, col), by = "com")
```


**Note:** This analysis refers the co-citation analysis, where the cited references and not the original publications are the unit of analysis. See tab `Technical description`for additional explanations

## Knowledge Bases summary

In order to partition networks into components or clusters, we deploy a **community detection** technique based on the **Lovain Algorithm** (Blondel et al., 2008). The Lovain Algorithm is a heuristic method that attempts to optimize the modularity of communities within a network by maximizing within- and minimizing between-community connectivity. We identify the following communities = knowledge bases.

```{r, include=FALSE}
kb_stats <- C_nw %>%
  group_by(label) %>%
  summarise(n = n(), density_int = ((sum(dgr_int) / (n() * (n() - 1))) * 100) %>% round(3)) %>%
  relocate(label, everything())
```

```{r}
kb_sum <-C_nw %>% group_by(com) %>% 
  arrange(com, desc(dgr_int)) %>%
  mutate(name = name %>% str_trunc(150)) %>%
  slice_max(order_by = dgr_int, n = 10, with_ties = FALSE) %>% 
  ungroup() %>%
  select( name, dgr_int, dgr) %>%
  kable() 

for(i in 1:nrow(com_names_cit)){
  kb_sum <- kb_sum %>%
    pack_rows(paste0('Knowledge Base ', i, ': ', com_names_cit[i, 'label'],
                     '   (n = ', kb_stats[i, 'n'], ', density =', kb_stats[i, 'density_int'] %>% round(2), ')' ), 
              (i*10-9),  (i*10), label_row_css = "background-color: #666; color: #fff;") 
  }

kb_sum %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 10)
```

## Development of Knowledge Bases

```{r, include=FALSE}
el_2m <- readRDS(paste0('../../temp/el_2m_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>%
  drop_na()
```


```{r, include=FALSE}
cit_com_year <- el_2m %>%
  count(com_cit, PY, name = 'TC') %>%
  group_by(PY) %>%
  mutate(TC_rel = TC / sum(TC)) %>%
  ungroup() %>%
  arrange(PY, com_cit) %>%
  left_join(com_names_cit , by = c('com_cit' = 'com')) %>% 
  complete(label, PY, fill = list(TC = 0, TC_rel = 0))
```

```{r, fig.width = 15, fig.height=7.5}
cit_com_year %>%
  plot_summary_timeline(y1 = TC, y2 = TC_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label, pal = pal_kb, label = TRUE,
                        y1_text = "Number citations recieved annually",  y2_text = "Share of citations recieved annually") +
  plot_annotation(title = paste('Knowledge Bses:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute knowledge base appearance (left), Relative knowledge base appearance (right)')
```

## Technical description
In a co-cittion network, the strength of the relationship between a reference pair $m$ and $n$ ($s_{m,n}^{coc}$) is expressed by the number of publications $C$ which are jointly citing reference $m$ and $n$. 

$$s_{m,n}^{coc} = \sum_i c_{i,m} c_{i,n}$$

The intuition here is that references which are frequently cited together are likely to share commonalities in theory, topic, methodology, or context. It can be interpreted as a measure of similarity as evaluated by other researchers that decide to jointly cite both references. Because the publication process is time-consuming, co-citation is a backward-looking measure, which is appropriate to map the relationship between core literature of a field.


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Research Areas: Bibliographic coupling analysis {.tabset}

## Research Areas main summary

This is arguably the more interesting part. Here, we identify the literature's current knowledge frontier by carrying out a bibliographic coupling analysis of the publications in our corpus. This measure  uses bibliographical information of  publications to establish a similarity relationship between them. Again, method details to be found in the tab `Technical description`. As you will see, we identify the main research area, but also a set of adjacent research areas with some theoretical/methodological/application overlap.

```{r, include=FALSE}
M_bib <- readRDS(paste0('../../temp/M_bib_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %>% 
  as_tibble() %>%
  drop_na(com)
```

```{r, include=FALSE}
com_names_bib <- tibble( 
  com = 1:(M_bib %>% pull(com) %>% n_distinct()),
  type = 'RA',
  col = com %>% gg_color_select(pal = pal_ra)) %>%
  left_join(com_labels %>% filter(type == 'RA') %>% select(com, label), by = 'com') %>%
  mutate(label = ifelse(is.na(label), paste0('RA ', com, ': unlabeled'), label))

# # 1st alternative: Number them 1-n
# paste(type, 1:(M_bib %>% pull(com) %>% n_distinct()))
```

```{r, include=FALSE}
M_bib %<>% left_join(com_names_bib %>% select(com, label, col), by = "com")
```

To identify communities in the field's knowledge frontier (labeled **research areas**) we again use the **Lovain Algorithm** (Blondel et al., 2008). We identify the following communities = research areas.

```{r, include=FALSE}
ra_stats <- M_bib %>%
  drop_na(com) %>%
  group_by(com, label) %>%
  summarise(n = n(), density_int = ((sum(dgr_int) / (n() * (n() - 1))) * 100) %>% round(3)) %>%
  select(com, label, everything())
```

```{r}
ra_sum <- M_bib %>% group_by(label) %>% 
  left_join(M %>% select(XX, AU, PY, TI, TC), by = 'XX') %>%
  mutate(dgr_select = (dgr_int / max(dgr_int) * (TC / max(TC))) ) %>%
  slice_max(order_by = dgr_select, n = 10, with_ties = FALSE) %>% 
  mutate(TC_year = (TC / (2021 + 1 - PY)) %>% round(2),
         dgr_int = dgr_int %>% round(2),
         AU = AU %>% str_trunc(25),
         TI = TI %>% str_trunc(125)) %>%
  ungroup() %>%
  select(AU, PY, TI, dgr_int, TC, TC_year) %>%
  kable()


for(i in 1:nrow(com_names_bib)){
  ra_sum  %<>%
    pack_rows(paste0('Research Area ', i, ': ', com_names_bib[i, 'label'],
                     '   (n = ', ra_stats[i, 'n'], ', density =', ra_stats[i, 'density_int'] %>% round(2), ')' ), 
              (i*10-9),  (i*10), label_row_css = "background-color: #666; color: #fff;") 
  }

ra_sum %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 10)
```

## Development

```{r, fig.width = 15, fig.height=7.5}
M_bib %>%
  left_join(M %>% select(XX, PY), by = 'XX') %>%
  mutate(PY = PY %>% as.numeric()) %>%
  group_by(label, PY) %>% summarise(n = n()) %>% ungroup() %>%
  group_by(PY) %>% mutate(n_PY = sum(n)) %>% ungroup() %>%
  mutate(n_rel = n / n_PY) %>%
  select(label, PY, n, n_rel) %>%
  arrange(label, PY) %>% 
  complete(label, PY, fill = list(n = 0, n_rel = 0)) %>%
  plot_summary_timeline(y1 = n, y2 = n_rel, t = PY, t_min = PY_min, t_max = PY_max, by = label, label = TRUE, pal = pal_ra,
                        y1_text = "Number publications annually", y2_text = "Share of publications annually") +
  plot_annotation(title = paste('Research Areas:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Absolute research area appearance (left), Relative research area appearance (right)')
```

### Connectivity between the research areas

```{r, include=FALSE}
g_agg <- readRDS(paste0('../../temp/g_bib_agg_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds')) %N>%
  arrange(com) # %>%
#   mutate(name = names_ra %>% pull(com_ra_name),
#          color = cols_ra)
```

```{r, fig.height= 7.5, fig.width=7.5}
g_agg %E>% 
  filter(weight > 0 & from != to) %>%
  filter(weight >= quantile(weight, 0.25) )  %N>%
  mutate(com = com_names_bib %>% pull(label)) %>%
  ggraph(layout = "circle") + 
  geom_edge_fan(strenght = 0.8, aes(width = weight), alpha = 0.2)  + 
  geom_node_point(aes(size = N, color = com))  + 
  geom_node_text(aes(label = com), repel = TRUE) +
  theme_graph(base_family = "Arial") +
  theme(legend.position = 'bottom') +
  scale_size(range = c(2,20)) +
  scale_color_brewer(palette = pal_ra) +
  labs(title = paste('Research Area Connectivity:', var_inst, 'Dept.', var_dept, sep = ' '),
                  subtitle = paste('Timeframe:', PY_min, '-', PY_max , sep = ' '),
                  caption = 'Nodes = Identified Research Areas; Edges: Bibliographic coupling strenght (Jaccard weighted)')
```

## Technical description
In a bibliographic coupling network, the **coupling-strength** between publications is determined by the number of commonly cited references they share, assuming a common pool of references to indicate similarity in context, methods, or theory. Formally, the strength of the relationship between a publication pair $i$ and $j$ ($s_{i,j}^{bib}$) is expressed by the number of commonly cited references. 

$$s_{i,j}^{bib} = \sum_m c_{i,m} c_{j,m}$$

Since our corpus contains publications which differ strongly in terms of the number of cited references, we normalize the coupling strength by the Jaccard similarity coefficient. Here, we weight the intercept of two publications' bibliography (shared refeences) by their union (number of all references cited by either $i$ or $j$). It is bounded between zero and one, where one indicates the two publications to have an identical bibliography, and zero that they do not share any cited reference. Thereby, we prevent publications from having high coupling strength due to a large bibliography (e.g., literature surveys).

$$S_{i,j}^{jac-bib} =\frac{C(i \cap j)}{C(i \cup j)} = \frac{s_{i,j}^{bib}}{c_i + c_j - s_{i,j}^{bib}}$$

More recent articles have a higher pool of possible references to co-cite to, hence they are more likely to be coupled. Consequently, bibliographic coupling represents a forward looking measure, and the method of choice to identify the current knowledge frontier at the point of analysis.

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Knowledge Bases, Research Areas & Topics Interaction

```{r, include=FALSE}
# Nodes
nl_3m <- com_names_bib %>%
  bind_rows(com_names_cit) %>%
  bind_rows(com_names_top) %>%
  rename(name = label,
         com_nr = com) %>%
  relocate(name)

# Edges
el_2m_kb <- el_2m %>%
  select(-from, -to) %>%
  inner_join(com_names_cit %>% select(com, label), by = c('com_cit' = 'com')) %>%
  inner_join(com_names_bib %>% select(com, label, col), by = c('com_bib' = 'com')) %>%
  mutate(weight = 1) %>%
  rename(from = label.x,
         to = label.y) %>% # generic
  select(from, to, weight, col) %>% 
  drop_na() %>% 
  count(from, to, col, wt = weight, name = 'weight') %>%
  filter(percent_rank(weight) >= 0.25) %>%
  weight_jaccard(i = from, j = to, w = weight) %>% 
  select(-weight)

el_2m_topic <- text_lda_gamma %>% select(-topic, -col) %>%
  left_join(M_bib %>% select(XX, com) %>% drop_na(com), by = c('document' = 'XX')) %>%
  inner_join(com_names_bib %>% select(com, label, col), by = c('com' = 'com')) %>%
  rename(from = label.y,
         to = label.x,
         weight = gamma) %>% # generic
  select(from, to, weight, col) %>% 
  drop_na() %>% 
  count(from, to, col, wt = weight, name = 'weight') %>%
  filter(percent_rank(weight) >= 0.25) %>%
  weight_jaccard(i = from, j = to, w = weight) %>% select(-weight)

# graph
g_3m <- el_2m_kb %>% 
  bind_rows(el_2m_topic) %>%
  as_tbl_graph(directed = TRUE) %N>%
  left_join(nl_3m, by = 'name') %>%
  mutate(
    level = case_when(
      type == "KB" ~ 1,
      type == "RA" ~ 2,
      type == "TP" ~ 3),
    coord_y = 0.1,
    coord_x = 0.001 + 1/(max(level)-1) * (level-1)
    )  %N>%
  filter(!is.na(level))
```

```{r, include=FALSE}
## Build sankey plot
fig <- plot_ly(type = "sankey", 
               orientation = "h",
               arrangement = "snap",
  node = list(
    label = g_3m %N>% as_tibble() %>% pull(name),
    x = g_3m %N>% as_tibble() %>% pull(coord_x),
    y = g_3m %N>% as_tibble() %>% pull(coord_y),
    color = g_3m %N>% as_tibble() %>% pull(col), 
    pad = 4
  ), 
  link = list(
    source = (g_3m %E>% as_tibble() %>% pull(from)) -1,
    target = (g_3m %E>% as_tibble() %>% pull(to)) -1,
    value =  g_3m %E>% as_tibble() %>% pull(weight_jac),
    color = g_3m %E>% as_tibble() %>% pull(col) %>% col2rgb() %>% as.matrix() %>% t() %>% as_tibble() %>% 
      mutate(col_rgb = paste0('rgba(', red, ',' , green, ',', blue, ',0.75)')) %>%  pull(col_rgb)
    )
) %>% 
  layout(title = paste('Knowledge Bases, Research Areas & Topics:', var_inst, 'Dept.', var_dept, sep = ' '),
         margin = list(l = 50, r = 50, b = 100, t = 100, pad = 2)) 
```

```{r, fig.height= 10, fig.width=12.5}
fig
```

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

<!--- 
# Department Position

## Collaaboration

```{r}
nw_inst <- M %>% as_tibble() %>% metaTagExtraction(Field = "AU_UN") %>% 
  select(XX, UT, PY, int_dept, AU_UN) %>% 
  # Add department as own entry in AU_UN
  mutate(AU_UN = ifelse(int_dept == TRUE, paste0(AU_UN, ';', var_inst, ' ', var_dept), AU_UN)) %>%
  # Sepperate AU_UN for 2_m edgelist
  separate_rows(AU_UN, sep = ';') %>%
  # filter
  drop_na(AU_UN) %>%
  filter(!(AU_UN %in% c('', ' ', 'NA', 'NOTREPORTED', 'NOTDECLARED'))) %>%
  # Only 1 link per paper, independent of author number
  distinct(UT, AU_UN, .keep_all = TRUE) 
```

```{r}
el_uni <- nw_inst %>% 
  left_join(nw_inst %>% select(UT, AU_UN), by = 'UT') %>% 
  rename(from = AU_UN.x, to = AU_UN.y) %>%
  filter(from != to) %>%
  group_by(from, to) %>%
  summarise(weight = n()) %>%
  ungroup()
```

```{r}
g_uni <- el_uni %>% as_tbl_graph(directed = FALSE) %E>%
  filter(weight >= cutof_edge_cit ) %N>%
  filter(!node_is_isolated())
```

  
g_cit <- g_cit %N>%
  mutate(dgr = centrality_degree(weights = weight)) %N>%
  filter(dgr >= cutof_node_cit) 

# Further restrictions
g_cit <- g_cit %N>% 
  filter(percent_rank(dgr) >= cutof_node_pct_cit) %E>%
  filter(percent_rank(weight) >= cutof_edge_pct_cit) %N>%
  filter(!node_is_isolated())


```{r}
g_uni %>%
  convert(to_local_neighborhood,
                     node = which(.N()$name == paste0(var_inst, ' ', var_dept)),
                     order = 1,
                     mode = "all") %>%
  ggraph(layout = "nicely") + 
  geom_edge_link(aes(width = weight), alpha = 0.2, col = 'lightgreen')  + 
  geom_node_point(aes(size = centrality_degree()), col = 'skyblue1')  + 
  geom_node_text(aes(label = name, 
                     size = centrality_degree(),
                     filter = centrality_degree() >= centrality_degree()  %>% quantile(0.6)), repel = TRUE) +
  theme_graph(base_family = "Arial") +
  theme(legend.position = 'bottom')
```

# Trends

## Topics

```{r}
# TODO: Solve earlier that it doesnt have to be loaded here again
el_sim_topic <- readRDS(paste0('../../temp/text_LDA_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.rds'))  %>% tidy(matrix = "gamma") %>%
  pairwise_similarity(document, topic, gamma, diag = FALSE, upper = TRUE)
```

```{r}
# join with year = uni
el_sim_topic %<>%
  left_join(M %>% select(XX, PY), by = c('item1' = 'XX')) %>%
  left_join(M %>% select(XX, PY), by = c('item2' = 'XX')) %>%
  rename(PY_from = PY.x, PY_to = PY.y)
```

```{r}
# decide for similarity past or future
el_sim_topic %<>%
  mutate(delta = PY_to - PY_from) %>%
  mutate(sim_type = case_when(
    delta == 0 ~ "present",
    delta >= 1 ~ "future",
    delta <= -1 ~ "past") 
    )
```


```{r}
# aggregate on document level
pub_sim <- el_sim_topic %>%
  group_by(item1, sim_type) %>%
  summarise(sim = mean(similarity)) %>%
  pivot_wider(names_from = sim_type, names_prefix = 'sim_', values_from = sim) %>%
  drop_na()
```

```{r}
uni_sim <- pub_sim %>%
  inner_join(nw_inst %>% select(XX, AU_UN), by = c('item1' = 'XX')) %>%
  group_by(AU_UN) %>%
  summarise(sim_past = mean(sim_past),
            sim_present = mean(sim_present),
            sim_future = mean(sim_future),
            n = n())
```

```{r}
uni_sim %>%
  slice_max(order_by = n, n = 100) %>%
  ggplot(aes(x = sim_past, y = sim_future)) +
  geom_point(aes(size = n), alpha = 0.25) +
  geom_text_repel( 
    data =uni_sim %>% slice_max(order_by = n, n = 5), # Filter data first
    aes(label = AU_UN),
    box.padding = 1, max.overlaps = Inf) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", col = 'snow3') 
# + lims(x = c(0.625, 0.725), y = c(0.625, 0.725))
```


--->

# Endnotes

All results are preliminary so far...

```{r}
# After knitted do this
#file.rename(from = "92_descriptives_mapping.nb.html", to = paste0('../output/field_mapping/field_mapping_', str_to_lower(var_inst), '_', str_to_lower(var_dept), '.html'))
```





